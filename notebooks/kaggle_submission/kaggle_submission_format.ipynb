{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 Open Research Dataset Challenge - What do we know about vaccines and therapuetics?\n",
    "The following questions were analysed specifically: \n",
    "- Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n",
    "  - Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\n",
    "- Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n",
    "- Exploration of use of best animal models and their predictive value for a human vaccine.\n",
    "- Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n",
    "- Efforts targeted at a universal coronavirus vaccine.\n",
    "- Efforts to develop animal models and standardize challenge studies\n",
    "- Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models (in conjunction with therapeutics)\n",
    "\n",
    "## Our approach - Creating a timeline visualizing the progress of vaccines/cures on COVID-19 and other similar viral diseases.\n",
    "Our goal is to create an intuitive visualization of the progress of research on vaccines and therapuetics regarding COVID-19. Not only is this useful for professional researchers in having a quick overview of the clinical trial stages of each investigated vaccine/therapeutic, but also for the public, to have a better understanding of the time frame for which to expect a cure or solution. We decided to create vizualizations of research progress of other virusses as well as COVID-19, to get a better picture of the timescale and ammount of research that goes into making a vaccine or therapeutics.\n",
    "\n",
    "Several steps were taken to create the visualizations:\n",
    "1. Load and preprocess the data:\n",
    "    - lemmatize all texts and remove stopwords\n",
    "2. Select papers containing words relevant to the research question\n",
    "    - using either string pattern matching or word embeddings\n",
    "    - relevant words were manually selected based on the research questions and indicativaty of clinical stage trial (e.g. mouse vs human test subject, words expressing certainty etc.)\n",
    "3. Extract keywords from selected papers\n",
    "    - TODO: write how we do this @Simon, @Silvan\n",
    "4. Extract links between selected papers\n",
    "    - TODO: write how we do this @Levi @Miguel\n",
    "5. Visualize extracted papers, links and summaries\n",
    "    - TODO: explain how (after we know how) @Levi @Gloria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.a Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# TODO: write your imports here\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# path to data\n",
    "data_dir = '../../src'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.b Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# As kaggle only allows notebook submissions, all functions should be in the notebook. Just copy your functions and paste them here.\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"COVID19 Papers Dataset\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir: string location where data files can be found.\n",
    "        paper_ids: list containing str of unique pdf ids for each paper. \n",
    "            ie. ['sha1', 'sha2', 'sha3', ...]\n",
    "        titles: list containing str titles of papers. \n",
    "            ie. ['title1', 'title2', 'title3', ...]\n",
    "        abstracts: list containing str abstracts of each paper. \n",
    "            ie. ['abstract1', 'abstract2', 'abstract3', ...]\n",
    "        n_paragraphs: list of integers specifying the amount of paragraphs in each paper. \n",
    "            ie. [n1, n2, n3, ...]\n",
    "        contents: nested list containing contents of paper; contents of each paper stored in a list of strings containing paragraphs. \n",
    "            ie. [['paper1_p1', 'paper1_p2', ...], ['paper2_p1', 'paper2_p2', ...], ...]\n",
    "    \n",
    "    Attributes are initially empty. To populate data, run class method of load_data().\n",
    "    \n",
    "    Usage:\n",
    "        # declare directory where data is stored\n",
    "        data_dir = '/kaggle/input/CORD-19-research-challenge'  \n",
    "        data = Dataset(data_dir)\n",
    "        data.load_data()\n",
    "        \n",
    "        # get attributes\n",
    "        data.paper_ids\n",
    "        data.titles\n",
    "        ...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir:str):\n",
    "        # init lists to store data\n",
    "        self.data_dir = data_dir\n",
    "        self.paper_ids = []\n",
    "        self.titles = []\n",
    "        self.abstracts = []\n",
    "        self.n_paragraphs = []\n",
    "        self.contents = []\n",
    "        \n",
    "        print(\"[INFO] Empty Dataset object created.\")\n",
    "        \n",
    "    @property\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples.\"\"\"\n",
    "        return f\"Dataset instance has {len(self.paper_ids)} samples\"\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load data from dataset data directory.\"\"\"\n",
    "        data_dir = str(self.data_dir)\n",
    "        subdir = [x for x in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,x))]\n",
    "        \n",
    "        initial_samples = len(self.paper_ids)\n",
    "        \n",
    "        print(f\"[INFO] Loading data from {data_dir}...\")\n",
    "        # loop through folders with json files\n",
    "        for folder in subdir:\n",
    "#             path = os.path.join(data_dir,folder, folder)\n",
    "            path = os.path.join(data_dir,folder, folder, 'pdf_json')\n",
    "            # loop through json files and scrape data\n",
    "            for file in os.listdir(path):\n",
    "                file_path = os.path.join(path, file)\n",
    "                \n",
    "                # open file only if it is a file\n",
    "                if os.path.isfile(file_path):\n",
    "                    with open(file_path) as f:\n",
    "                        data_json = json.load(f)\n",
    "                        self.paper_ids.append(data_json['paper_id'])\n",
    "                        self.titles.append(data_json['metadata']['title'])\n",
    "\n",
    "                        # combine abstract texts / process\n",
    "                        combined_str = ''\n",
    "                        for text in data_json['abstract']:\n",
    "                            combined_str += text['text'].lower()\n",
    "\n",
    "                        self.abstracts.append(combined_str)\n",
    "\n",
    "                        # take only text part for content\n",
    "                        paragraphs = []\n",
    "                        content = data_json['body_text']\n",
    "\n",
    "                        for paragraph in content:\n",
    "                            paragraphs.append(paragraph['text'].lower())\n",
    "\n",
    "                        self.n_paragraphs.append(len(content))\n",
    "                        self.contents.append(paragraphs) \n",
    "                else:\n",
    "                    print('[WARNING]', file_path, 'not a file. Check pointed path directory in load_data().')\n",
    "        \n",
    "        end_samples = len(self.paper_ids)\n",
    "        loaded_samples = end_samples - initial_samples\n",
    "        print(f\"[INFO] Data loaded into dataset instance. {loaded_samples} samples added. | Start amount = {initial_samples}; End amount = {end_samples}\")\n",
    "\n",
    "        \n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize \n",
    "\n",
    "def tokenize_check(text):\n",
    "    if isinstance(text, str):\n",
    "        word_tokens = word_tokenize(text)\n",
    "    elif isinstance(text, list):\n",
    "        word_tokens = text\n",
    "    else:\n",
    "        raise TypeError\n",
    "    \n",
    "    return word_tokens\n",
    "    \n",
    "\n",
    "def remove_stopwords(text, remove_symbols=False):\n",
    "    \"\"\" Tokenize and/or remove stopwords and/or unwanted symbols from string\"\"\"\n",
    "    list_stopwords = set(stopwords.words('english'))\n",
    "    # list of signs to be removed if parameter remove_symbols set to True\n",
    "    list_symbols = ['.', ',', '(', ')', '[', ']']\n",
    "    \n",
    "    # check input type and tokenize if not already\n",
    "    word_tokens = tokenize_check(text)\n",
    "\n",
    "    # filter out stopwords\n",
    "    text_without_stopwords = [w for w in word_tokens if not w in list_stopwords] \n",
    "    \n",
    "    if remove_symbols is True:\n",
    "        text_without_stopwords = [w for w in text_without_stopwords if not w in list_symbols]\n",
    "    \n",
    "    return text_without_stopwords\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\" Tokenize and/or lemmatize string \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # check input type and tokenize if not already\n",
    "    word_tokens = tokenize_check(text)\n",
    "    \n",
    "    lemmatized_text = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "def select_papers(data, virus_strings, clinical_stage_strings):\n",
    "    #TODO @Pooja @Miguel: select papers based on relevant_strings\n",
    "\n",
    "    # output: selected papers + the strings that were found in these papers\n",
    "    selected_papers['found_substrings'] = found_substrings\n",
    "    return selected_papers\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # TODO @Simon @Silvan: extract keywords\n",
    "    return keywords\n",
    "\n",
    "def extract_links(data):\n",
    "    # TODO @Levi @Miguel: extract links between papers    \n",
    "    return links\n",
    "\n",
    "#def visualize_data(data,keywords,summaries):\n",
    "    #TODO @Levi @Kwan: visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.c Relevant strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_keywords = pd.read_csv(data_dir+'/keywords/virus_keywords.csv')\n",
    "clinical_stage_keywords = pd.read_csv(data_dir+'/keywords/phase_keywords.csv')\n",
    "\n",
    "## TODO: create this csv file:\n",
    "#question_keywords = pd.read_csv('../content/question_keywords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Empty Dataset object created.\n",
      "[INFO] Loading data from ../../src/CORD-19-research-challenge...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../src/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-bcd41ad1ad69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create dataset object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/CORD-19-research-challenge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# add colums with processed text (no stopwords, lemmatized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-de78a1ba557d>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pdf_json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;31m# loop through json files and scrape data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../src/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json'"
     ]
    }
   ],
   "source": [
    "# create dataset object\n",
    "data = Dataset(data_dir+'/CORD-19-research-challenge')\n",
    "data.load_data()\n",
    "\n",
    "# add colums with processed text (no stopwords, lemmatized)\n",
    "processed_text = data['text'].apply(remove_stopwords())\n",
    "processed_text = processed_text.apply(lemmatize())\n",
    "\n",
    "# append processed text to data\n",
    "data['processed_text'] = processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Select papers containing words relevant to the research question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_papers = select_papers(data, virus_strings, clinical_stage_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract keywords from selected papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = extract_keywords(selected_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract links between selected papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0, 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5f2894e9f20f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_index_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"get_indexer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._check_type\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0, 0)"
     ]
    }
   ],
   "source": [
    "paper_links = extract_links(selected_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualize extracted papers, links and summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data(selected_papers,keywords,paper_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
