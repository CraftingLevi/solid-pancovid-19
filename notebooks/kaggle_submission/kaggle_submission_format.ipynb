{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 Open Research Dataset Challenge - What do we know about vaccines and therapuetics?\n",
    "The following questions were analysed specifically: \n",
    "- Effectiveness of drugs being developed and tried to treat COVID-19 patients.\n",
    "  - Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\n",
    "- Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n",
    "- Exploration of use of best animal models and their predictive value for a human vaccine.\n",
    "- Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n",
    "- Efforts targeted at a universal coronavirus vaccine.\n",
    "- Efforts to develop animal models and standardize challenge studies\n",
    "- Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models (in conjunction with therapeutics)\n",
    "\n",
    "## Our approach - Creating a timeline visualizing the progress of vaccines/cures on COVID-19 and other similar viral diseases.\n",
    "Our goal is to create an intuitive visualization of the progress of research on vaccines and therapuetics regarding COVID-19. Not only is this useful for professional researchers in having a quick overview of the clinical trial stages of each investigated vaccine/therapeutic, but also for the public, to have a better understanding of the time frame for which to expect a cure or solution. We decided to create vizualizations of research progress of other virusses as well as COVID-19, to get a better picture of the timescale and ammount of research that goes into making a vaccine or therapeutics.\n",
    "\n",
    "Several steps were taken to create the visualizations:\n",
    "1. Load and preprocess the data:\n",
    "    - lemmatize all texts and remove stopwords\n",
    "2. Select papers containing words relevant to the research question\n",
    "    - using either string pattern matching or word embeddings\n",
    "    - relevant words were manually selected based on the research questions and indicativaty of clinical stage trial (e.g. mouse vs human test subject, words expressing certainty etc.)\n",
    "3. Extract keywords from selected papers\n",
    "    - TODO: write how we do this @Simon, @Silvan\n",
    "4. Extract links between selected papers\n",
    "    - TODO: write how we do this @Levi @Miguel\n",
    "5. Visualize extracted papers, links and summaries\n",
    "    - TODO: explain how (after we know how) @Levi @Gloria\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.a Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# TODO: write your imports here\n",
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# path to data\n",
    "data_dir = '../../src'  \n",
    "keyword_dir = '../../keywords'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.b Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# As kaggle only allows notebook submissions, all functions should be in the notebook. Just copy your functions and paste them here.\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"COVID19 Papers Dataset\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir: string location where data files can be found.\n",
    "        paper_ids: list containing str of unique pdf ids for each paper. \n",
    "            ie. ['sha1', 'sha2', 'sha3', ...]\n",
    "        titles: list containing str titles of papers. \n",
    "            ie. ['title1', 'title2', 'title3', ...]\n",
    "        abstracts: list containing str abstracts of each paper. \n",
    "            ie. ['abstract1', 'abstract2', 'abstract3', ...]\n",
    "        n_paragraphs: list of integers specifying the amount of paragraphs in each paper. \n",
    "            ie. [n1, n2, n3, ...]\n",
    "        contents: nested list containing contents of paper; contents of each paper stored in a list of strings containing paragraphs. \n",
    "            ie. [['paper1_p1', 'paper1_p2', ...], ['paper2_p1', 'paper2_p2', ...], ...]\n",
    "    \n",
    "    Attributes are initially empty. To populate data, run class method of load_data().\n",
    "    \n",
    "    Usage:\n",
    "        # declare directory where data is stored\n",
    "        data_dir = '/kaggle/input/CORD-19-research-challenge'  \n",
    "        data = Dataset(data_dir)\n",
    "        data.load_data()\n",
    "        \n",
    "        # get attributes\n",
    "        data.paper_ids\n",
    "        data.titles\n",
    "        ...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir:str):\n",
    "        # init lists to store data\n",
    "        self.data_dir = data_dir\n",
    "        self.paper_ids = []\n",
    "        self.titles = []\n",
    "        self.abstracts = []\n",
    "        self.n_paragraphs = []\n",
    "        self.contents = []\n",
    "        \n",
    "        print(\"[INFO] Empty Dataset object created.\")\n",
    "        \n",
    "    @property\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples.\"\"\"\n",
    "        return f\"Dataset instance has {len(self.paper_ids)} samples\"\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load data from dataset data directory.\"\"\"\n",
    "        data_dir = str(self.data_dir)\n",
    "        subdir = [x for x in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,x))]\n",
    "        \n",
    "        initial_samples = len(self.paper_ids)\n",
    "        \n",
    "        print(f\"[INFO] Loading data from {data_dir}...\")\n",
    "        # loop through folders with json files\n",
    "        for folder in subdir:\n",
    "            \n",
    "#             path = os.path.join(data_dir,folder, folder)\n",
    "            # path = os.path.join(data_dir,folder, folder, 'pdf_json')\n",
    "            path = os.path.join(data_dir,folder, folder)\n",
    "            # loop through json files and scrape data\n",
    "            for file in os.listdir(path):\n",
    "                file_path = os.path.join(path, file)\n",
    "                \n",
    "                # open file only if it is a file\n",
    "                if os.path.isfile(file_path):\n",
    "                    with open(file_path) as f:\n",
    "                        try:\n",
    "                            data_json = json.load(f)\n",
    "                        except:\n",
    "                            # Broken file\n",
    "                            self.paper_ids.append([])\n",
    "                            self.titles.append([])\n",
    "                            self.abstracts.append([])\n",
    "                            self.n_paragraphs.append(None)\n",
    "                            self.contents.append([])\n",
    "                        else:\n",
    "                            self.paper_ids.append(data_json['paper_id'])\n",
    "                            self.titles.append(data_json['metadata']['title'])\n",
    "\n",
    "                            # combine abstract texts / process\n",
    "                            combined_str = ''\n",
    "                            for text in data_json['abstract']:\n",
    "                                combined_str += text['text'].lower()\n",
    "\n",
    "                            self.abstracts.append(combined_str)\n",
    "\n",
    "                            # take only text part for content\n",
    "                            paragraphs = []\n",
    "                            content = data_json['body_text']\n",
    "\n",
    "                            for paragraph in content:\n",
    "                                paragraphs.append(paragraph['text'].lower())\n",
    "\n",
    "                            self.n_paragraphs.append(len(content))\n",
    "                            self.contents.append(paragraphs)\n",
    "                else:\n",
    "                    print('[WARNING]', file_path, 'not a file. Check pointed path directory in load_data().')\n",
    "        \n",
    "        end_samples = len(self.paper_ids)\n",
    "        loaded_samples = end_samples - initial_samples\n",
    "        print(f\"[INFO] Data loaded into dataset instance. {loaded_samples} samples added. | Start amount = {initial_samples}; End amount = {end_samples}\")\n",
    "\n",
    "        \n",
    "def tokenize_check(text):\n",
    "    if isinstance(text, str):\n",
    "        word_tokens = word_tokenize(text)\n",
    "    elif isinstance(text, list):\n",
    "        word_tokens = text\n",
    "    else:\n",
    "        raise TypeError\n",
    "    \n",
    "    return word_tokens\n",
    "    \n",
    "\n",
    "def remove_stopwords(text, remove_symbols=False):\n",
    "    \"\"\" Tokenize and/or remove stopwords and/or unwanted symbols from string\"\"\"\n",
    "    list_stopwords = set(stopwords.words('english'))\n",
    "    # list of signs to be removed if parameter remove_symbols set to True\n",
    "    list_symbols = ['.', ',', '(', ')', '[', ']']\n",
    "    \n",
    "    # check input type and tokenize if not already\n",
    "    word_tokens = tokenize_check(text)\n",
    "\n",
    "    # filter out stopwords\n",
    "    text_without_stopwords = [w for w in word_tokens if not w in list_stopwords] \n",
    "    \n",
    "    if remove_symbols is True:\n",
    "        text_without_stopwords = [w for w in text_without_stopwords if not w in list_symbols]\n",
    "    \n",
    "    return text_without_stopwords\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\" Tokenize and/or lemmatize string \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # check input type and tokenize if not already\n",
    "    word_tokens = tokenize_check(text)\n",
    "    \n",
    "    lemmatized_text = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
    "    \n",
    "    return lemmatized_text\n",
    "\n",
    "def select_papers(data, df):\n",
    "    \"\"\" Find relevant papers for the categories in df\n",
    "    Returns a dictionary with the paper id's that match the categories\n",
    "    It also stores the sentences where the matches have been found. This can be returned too if so the team decides \"\"\"\n",
    "\n",
    "    # Dictionary to be returned: contains shas and sentences for each key word or key phrase\n",
    "    matches = {k: {} for k in df.columns}\n",
    "\n",
    "    # Data cleaning:\n",
    "    # Turn df into a dictionary with a list of key phrases\n",
    "    # Lower all of them and remove null values\n",
    "    dfd = {k: [x.lower() for x in v if not pd.isnull(x)] for k, v in df.to_dict('list').items()}\n",
    "    # Remove redundant values (i.e., ['coronavirus', 'coronavirus disease'] can be left as ['coronavirus']; the element 'coronavirus disease' is useless)\n",
    "    for k, v in dfd.items():\n",
    "        # print(k)\n",
    "        v = [x for x in v if not any([y in x for y in [z for z in v if z != x]])]\n",
    "        dfd[k] = v\n",
    "\n",
    "        # Find matches\n",
    "        # Use the loop we're in where we've already cleaned the data to find the matches\n",
    "        for sha, text in zip(data.paper_ids, data.contents):\n",
    "            for paragraph in text:\n",
    "                for sentence in sent_tokenize(paragraph):\n",
    "                    for keyphrase in v:\n",
    "                        if keyphrase in sentence:\n",
    "                            try:\n",
    "                                already_a_match = sentence in matches[k][sha]\n",
    "                            except KeyError:\n",
    "                                matches[k][sha] = [sentence]\n",
    "                            else:\n",
    "                                if not already_a_match:\n",
    "                                    matches[k][sha].append(sentence)\n",
    "\n",
    "    # Paper id's only\n",
    "    paper_ids = {k: list(v.keys()) for k, v in matches.items()}\n",
    "\n",
    "    # For now, return paper_ids only\n",
    "    return paper_ids\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # TODO @Simon @Silvan: extract keywords\n",
    "    return keywords\n",
    "\n",
    "def extract_links(data):\n",
    "    # TODO @Levi @Miguel: extract links between papers    \n",
    "    return links\n",
    "\n",
    "#def visualize_data(data,keywords,summaries):\n",
    "#    #TODO @Levi @Kwan: visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.c Relevant strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords that define the virus the paper is about (likely in title)\n",
    "virus_keywords = pd.read_csv(keyword_dir+'/virus_keywords.csv')\n",
    "\n",
    "# keywords describing clinical phase\n",
    "clinical_stage_keywords = pd.read_csv(keyword_dir+'/phase_keywords.csv')\n",
    "\n",
    "# keywords describing treatment types\n",
    "drug_keywords = pd.read_csv(keyword_dir+'/drug_keywords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Empty Dataset object created.\n",
      "[INFO] Loading data from ../../src...\n",
      "[INFO] Data loaded into dataset instance. 20 samples added. | Start amount = 0; End amount = 20\n"
     ]
    }
   ],
   "source": [
    "# create dataset object\n",
    "data = Dataset(data_dir)\n",
    "\n",
    "# load data\n",
    "data.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Select papers containing words relevant to the research question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "viruses\n",
      "\t * COVID-19: ['f7b069911c90d9deab85becbad3e9a633e0bf57c', '05d47dd5b46f86428de058db4ecc8bca76a9ad16', '4f53e43ba1bfb84611eaa839994f9297cdd65dc9']\n",
      "\t * common cold: ['fb968c1fb602f50dd434c8a7cb28129c3a7a4217', 'fd3e7ddb2fd67617369e927b01caf03a5747f675', 'c61dff4c277d87714af42ea265f73fcb1a74af84']\n",
      "\t * SARS-CoV (2003): ['945b14460fc5e5d75092e4dd06bb09be03f10118', '05d47dd5b46f86428de058db4ecc8bca76a9ad16']\n",
      "\t * HCoV NL63 (2004): []\n",
      "\t * HKU1 (2005): []\n",
      "\t * MERS-CoV (2012): ['69ca8c0a79935fbe985f02fc53c1a9da5c42acd3', '945b14460fc5e5d75092e4dd06bb09be03f10118', '05d47dd5b46f86428de058db4ecc8bca76a9ad16', '147422b51b66a3e9c76f32fab4b4a004f67e8a16']\n",
      "-------------------------\n",
      "phases\n",
      "\t * preclinical: ['40b2b0e3d2cf519723d858a138e77a26a70f460f', 'ccb533d872f914aee01fd533f330c6d768da14c5', '69ca8c0a79935fbe985f02fc53c1a9da5c42acd3', '945b14460fc5e5d75092e4dd06bb09be03f10118', 'fb968c1fb602f50dd434c8a7cb28129c3a7a4217', '05d47dd5b46f86428de058db4ecc8bca76a9ad16', '26cb6703ca72bf97887abbc29a40b1bd9d7890f4', '4f53e43ba1bfb84611eaa839994f9297cdd65dc9', 'dbf76534e0b330bb0f155296e251c1e7031c90c0', '147422b51b66a3e9c76f32fab4b4a004f67e8a16', '1ef024725b588f435abcc6c51189d14b223d24fa', '2a8e26038c98efac0a61629aa6b768fefad5a573', '54f0691d25999d426e30f0d324eef7ee08b53ab2', '1ee58e5747a525a9c1bbe7367f3da6f69f1c9b56', '00683d59d56123ae85e080d00ef1b3edd3f7405d', 'fd3e7ddb2fd67617369e927b01caf03a5747f675']\n",
      "\t * Phase 0: ['ccb533d872f914aee01fd533f330c6d768da14c5', '945b14460fc5e5d75092e4dd06bb09be03f10118', 'fb968c1fb602f50dd434c8a7cb28129c3a7a4217', 'f7b069911c90d9deab85becbad3e9a633e0bf57c', '05d47dd5b46f86428de058db4ecc8bca76a9ad16', '26cb6703ca72bf97887abbc29a40b1bd9d7890f4', '180c88b68816bbb6eb57aa515a3aec4020178729', '4f53e43ba1bfb84611eaa839994f9297cdd65dc9', 'dbf76534e0b330bb0f155296e251c1e7031c90c0', '147422b51b66a3e9c76f32fab4b4a004f67e8a16', 'c5b4fb9b393c9e5d24b075349fbb4601ea89c8f4', '1ef024725b588f435abcc6c51189d14b223d24fa', '2a8e26038c98efac0a61629aa6b768fefad5a573', '54f0691d25999d426e30f0d324eef7ee08b53ab2', '1ee58e5747a525a9c1bbe7367f3da6f69f1c9b56', '00683d59d56123ae85e080d00ef1b3edd3f7405d']\n",
      "\t * Phase 1: ['40b2b0e3d2cf519723d858a138e77a26a70f460f', 'ccb533d872f914aee01fd533f330c6d768da14c5', '69ca8c0a79935fbe985f02fc53c1a9da5c42acd3', '945b14460fc5e5d75092e4dd06bb09be03f10118', 'fb968c1fb602f50dd434c8a7cb28129c3a7a4217', 'f7b069911c90d9deab85becbad3e9a633e0bf57c', '05d47dd5b46f86428de058db4ecc8bca76a9ad16', '26cb6703ca72bf97887abbc29a40b1bd9d7890f4', '180c88b68816bbb6eb57aa515a3aec4020178729', '4f53e43ba1bfb84611eaa839994f9297cdd65dc9', 'dbf76534e0b330bb0f155296e251c1e7031c90c0', '147422b51b66a3e9c76f32fab4b4a004f67e8a16', '1ef024725b588f435abcc6c51189d14b223d24fa', '2a8e26038c98efac0a61629aa6b768fefad5a573', '1ee58e5747a525a9c1bbe7367f3da6f69f1c9b56', 'fd3e7ddb2fd67617369e927b01caf03a5747f675', 'c61dff4c277d87714af42ea265f73fcb1a74af84']\n",
      "\t * Phase 2: ['40b2b0e3d2cf519723d858a138e77a26a70f460f', 'ccb533d872f914aee01fd533f330c6d768da14c5', '69ca8c0a79935fbe985f02fc53c1a9da5c42acd3', 'fb968c1fb602f50dd434c8a7cb28129c3a7a4217', 'f7b069911c90d9deab85becbad3e9a633e0bf57c', '05d47dd5b46f86428de058db4ecc8bca76a9ad16', '26cb6703ca72bf97887abbc29a40b1bd9d7890f4', '4f53e43ba1bfb84611eaa839994f9297cdd65dc9', 'dbf76534e0b330bb0f155296e251c1e7031c90c0', '147422b51b66a3e9c76f32fab4b4a004f67e8a16', '00683d59d56123ae85e080d00ef1b3edd3f7405d', 'c61dff4c277d87714af42ea265f73fcb1a74af84']\n",
      "\t * Phase 3: ['40b2b0e3d2cf519723d858a138e77a26a70f460f', 'ccb533d872f914aee01fd533f330c6d768da14c5', '69ca8c0a79935fbe985f02fc53c1a9da5c42acd3', '945b14460fc5e5d75092e4dd06bb09be03f10118', 'fb968c1fb602f50dd434c8a7cb28129c3a7a4217', 'f7b069911c90d9deab85becbad3e9a633e0bf57c', '05d47dd5b46f86428de058db4ecc8bca76a9ad16', '26cb6703ca72bf97887abbc29a40b1bd9d7890f4', '4f53e43ba1bfb84611eaa839994f9297cdd65dc9', 'dbf76534e0b330bb0f155296e251c1e7031c90c0', '147422b51b66a3e9c76f32fab4b4a004f67e8a16', 'c5b4fb9b393c9e5d24b075349fbb4601ea89c8f4', '1ee58e5747a525a9c1bbe7367f3da6f69f1c9b56', 'fd3e7ddb2fd67617369e927b01caf03a5747f675', 'c61dff4c277d87714af42ea265f73fcb1a74af84']\n",
      "\t * Phase 4: ['40b2b0e3d2cf519723d858a138e77a26a70f460f', 'ccb533d872f914aee01fd533f330c6d768da14c5', 'fb968c1fb602f50dd434c8a7cb28129c3a7a4217', 'f7b069911c90d9deab85becbad3e9a633e0bf57c', '05d47dd5b46f86428de058db4ecc8bca76a9ad16', '26cb6703ca72bf97887abbc29a40b1bd9d7890f4', '4f53e43ba1bfb84611eaa839994f9297cdd65dc9', 'dbf76534e0b330bb0f155296e251c1e7031c90c0', '147422b51b66a3e9c76f32fab4b4a004f67e8a16', '1ee58e5747a525a9c1bbe7367f3da6f69f1c9b56', '00683d59d56123ae85e080d00ef1b3edd3f7405d']\n",
      "-------------------------\n",
      "drugs\n",
      "\t * antiviral drugs: ['fb968c1fb602f50dd434c8a7cb28129c3a7a4217', '05d47dd5b46f86428de058db4ecc8bca76a9ad16', '147422b51b66a3e9c76f32fab4b4a004f67e8a16', '2a8e26038c98efac0a61629aa6b768fefad5a573', '1ee58e5747a525a9c1bbe7367f3da6f69f1c9b56']\n",
      "\t * less common viral inhibitors: []\n",
      "\t * therapeutics: ['ccb533d872f914aee01fd533f330c6d768da14c5', 'fb968c1fb602f50dd434c8a7cb28129c3a7a4217', '05d47dd5b46f86428de058db4ecc8bca76a9ad16', '26cb6703ca72bf97887abbc29a40b1bd9d7890f4', '147422b51b66a3e9c76f32fab4b4a004f67e8a16', '1ee58e5747a525a9c1bbe7367f3da6f69f1c9b56', 'c61dff4c277d87714af42ea265f73fcb1a74af84']\n",
      "\t * vaccine: ['945b14460fc5e5d75092e4dd06bb09be03f10118', 'fb968c1fb602f50dd434c8a7cb28129c3a7a4217', '05d47dd5b46f86428de058db4ecc8bca76a9ad16', 'dbf76534e0b330bb0f155296e251c1e7031c90c0', '147422b51b66a3e9c76f32fab4b4a004f67e8a16', 'c5b4fb9b393c9e5d24b075349fbb4601ea89c8f4', '00683d59d56123ae85e080d00ef1b3edd3f7405d']\n"
     ]
    }
   ],
   "source": [
    "selected_papers = {}\n",
    "selected_papers['viruses'] = select_papers(data, virus_keywords)\n",
    "selected_papers['phases'] = select_papers(data, clinical_stage_keywords)\n",
    "selected_papers['drugs'] = select_papers(data, drug_keywords)\n",
    "for k, v in selected_papers.items():\n",
    "    print('-------------------------')\n",
    "    print(k)\n",
    "    for kw, sha_hashes in v.items():\n",
    "        print(\"\\t * %s: %s\"%(kw, sha_hashes))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract keywords from selected papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-85be3d8428fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_papers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-95f4b16ddbd6>\u001b[0m in \u001b[0;36mextract_keywords\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# TODO @Simon @Silvan: extract keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_links\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keywords' is not defined"
     ]
    }
   ],
   "source": [
    "keywords = extract_keywords(selected_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract links between selected papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paper_links = extract_links(selected_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualize extracted papers, links and summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data(selected_papers,keywords,paper_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
